# Transformer Exploration for News Summarization :newspaper:

This project explores Transformer-based models inspired by the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762). We compare three approaches for **news article summarization** using the [Kaggle News Summarization dataset](https://www.kaggle.com/datasets/sbhatti/news-summarization):

1. :zap: A **Transformer** model (as in *Attention Is All You Need*)  

---

## üìÅ Structure du projet

- `src/` : Contient l'ensemble des scripts Python n√©cessaires √† l'entra√Ænement, au pr√©-traitement et √† l'√©valuation de notre mod√®le.
- `train_test.py` *(√† la racine)* : Permet d'entra√Æner le mod√®le.
- `app/api.py` : Lance une API pour interagir avec le mod√®le entra√Æn√©.


## üì¶ Donn√©es et poids du mod√®le

- **Donn√©es** : H√©berg√©es publiquement sur un serveur **S3** (donn√©es brutes et donn√©es nettoy√©es).
- **Poids du mod√®le** : Disponibles sur **HuggingFace** et automatiquement r√©cup√©r√©s lors du lancement de l‚ÄôAPI.


## üöÄ Installation

1. **Cloner le d√©p√¥t :**

```bash
git clone https://github.com/nico5655/transformer-news-summarization.git
cd transformer-news-summarization
```
### Cr√©er un environnement virtuel
```bash
python -m venv env
```
### Activer l‚Äôenvironnement virtuel
```bash
source env/bin/activate  # Sous Windows : env\Scripts\activate
```
### Installer les d√©pendances
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```
---

## Nous mettons les r√©sultats de la performance de notre mod√®le ici : 

The models were trained on an NVIDIA A100 GPU with 40 GB of high-bandwidth memory :computer:

| Model                        | ROUGE-1 | ROUGE-2 | ROUGE-L | Train Time (ep)  | Params  |
|------------------------------|---------|---------|---------|------------------|---------|
| Transformer                  | 0.20    | 0.04    | 0.15    | ~ $1.6 \times 10^4$ s (25) | $1.25 \times 10^7$  |





